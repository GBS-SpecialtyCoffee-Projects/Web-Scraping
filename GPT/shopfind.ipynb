{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b757af-724b-448a-ad1a-a0162ba21bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib3\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ============================\n",
    "# CONFIGURATION & CONSTANTS\n",
    "# ============================\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "client = AzureOpenAI(\n",
    "    api_key=\"your openai key\",\n",
    ")\n",
    "\n",
    "GOOGLE_API_KEY   = 'your GCP api key'\n",
    "BASE_URL_SEARCH  = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "BASE_URL_DETAILS = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "\n",
    "QUERIES = [\n",
    "    'specialty coffee roaster',\n",
    "    'third wave coffee roaster',\n",
    "    'artisan coffee roaster',\n",
    "    \n",
    "]\n",
    "\n",
    "CITIES    = ['Huntsville','Des Moines','Allentown','Modesto','Syracuse'] #a list of different cities\n",
    "REGION    = 'us'\n",
    "MAX_DEPTH = 5\n",
    "\n",
    "# ============================\n",
    "# SESSION SETUP WITH RETRIES\n",
    "# ============================\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    return session\n",
    "\n",
    "session = create_session()\n",
    "\n",
    "# ============================\n",
    "# GOOGLE PLACES HELPERS\n",
    "# ============================\n",
    "def get_places_data(query, region, next_page_token=None):\n",
    "    params = {'query': query, 'region': region, 'key': GOOGLE_API_KEY}\n",
    "    if next_page_token:\n",
    "        params['pagetoken'] = next_page_token\n",
    "    try:\n",
    "        response = session.get(BASE_URL_SEARCH, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for query '{query}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_place_details(place_id):\n",
    "    params = {\n",
    "        'place_id': place_id,\n",
    "        'fields': 'name,formatted_address,website',\n",
    "        'key': GOOGLE_API_KEY\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(BASE_URL_DETAILS, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching details for place ID '{place_id}': {e}\")\n",
    "        return None\n",
    "\n",
    "def canonicalize_website(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc.lower()\n",
    "        if domain.startswith(\"www.\"):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing URL '{url}': {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_place_info(place):\n",
    "    pid     = place.get('place_id')\n",
    "    name    = place.get('name')\n",
    "    address = place.get('formatted_address', 'No address provided')\n",
    "    details = get_place_details(pid)\n",
    "    website = 'No website provided'\n",
    "    if details and 'result' in details:\n",
    "        website = details['result'].get('website', website)\n",
    "    canon = canonicalize_website(website) if website != 'No website provided' else None\n",
    "    return pid, name, address, website, canon\n",
    "\n",
    "# ============================\n",
    "# SCRAPE RAW DATA FOR ONE CITY\n",
    "# ============================\n",
    "def scrape_google_places_for_city(city):\n",
    "    all_places   = []\n",
    "    unique_names = set()\n",
    "    unique_webs  = set()\n",
    "\n",
    "    for q in QUERIES:\n",
    "        full_query = f\"{city} {q}\"\n",
    "        print(f\"\\nSearching for '{full_query}' in {city}...\")\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            data = get_places_data(full_query, REGION, next_page_token)\n",
    "            if not data or 'results' not in data:\n",
    "                break\n",
    "            for place in data['results']:\n",
    "                pid, name, addr, web, canon = extract_place_info(place)\n",
    "                if (web == 'No website provided'\n",
    "                        or name in unique_names\n",
    "                        or (canon and canon in unique_webs)):\n",
    "                    continue\n",
    "                unique_names.add(name)\n",
    "                if canon:\n",
    "                    unique_webs.add(canon)\n",
    "                all_places.append((name, addr, web))\n",
    "                print(f\"  Added: {name}, {addr}, {web}\")\n",
    "                time.sleep(1)\n",
    "            next_page_token = data.get('next_page_token')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            time.sleep(2)\n",
    "\n",
    "    raw_csv = f\"{city}_specialty_coffee_roasters.csv\"\n",
    "    with open(raw_csv, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Name,Address,Website\\n\")\n",
    "        for n, a, w in all_places:\n",
    "            safe_n = n.replace('\"', '\"\"')\n",
    "            safe_a = a.replace('\"', '\"\"')\n",
    "            safe_w = w.replace('\"', '\"\"')\n",
    "            f.write(f'\"{safe_n}\",\"{safe_a}\",\"{safe_w}\"\\n')\n",
    "\n",
    "    print(f\"[Saved] raw data → {raw_csv}\")\n",
    "    return raw_csv\n",
    "\n",
    "# ============================\n",
    "# GPT-DRIVEN ITERATIVE LINK-FINDING\n",
    "# ============================\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'a'], limit=50)\n",
    "        text = ' '.join(el.get_text(strip=True) for el in elements)\n",
    "        return text[:3000], soup\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def analyze_with_gpt(website_text, address, city):\n",
    "    print(website_text)\n",
    "    prompt = (\n",
    "        f\"Based on the following content from this website and the provided address, \"\n",
    "        f\"please answer the following questions:\\n\\n\"\n",
    "        f\"Website Content: {website_text}\\n\\n\"\n",
    "        f\"Address: {address}\\n\\n\"\n",
    "        f\"1. Does the site offer a shop where users can buy coffee beans? Provide a yes or no answer without explanation.\\n\"\n",
    "        f\"2. Is the business genuinely located in {city} or near it based on the address and website content? (If address not provided, assume yes.)\"\n",
    "    )\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3, max_tokens=500\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip().lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with GPT request: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def verify_page_sells_coffee_gpt(soup, base_url):\n",
    "    anchors = soup.find_all('a', href=True)[:100]\n",
    "    if not anchors:\n",
    "        return False\n",
    "    lines = []\n",
    "    for a in anchors:\n",
    "        text = a.get_text(strip=True)\n",
    "        full = urljoin(base_url, a['href'])\n",
    "        lines.append(f\"Text: {text}, URL: {full}\")\n",
    "    prompt = (\n",
    "    \"Below is a list of anchor tags (text and URL) from a webpage. \"\n",
    "    \"We want to determine if this page serves as the main shopping hub for coffee beans—\"\n",
    "    \"that is, a page aggregating many links to individual product listings. \"\n",
    "    \"If it’s just a single product page, an about page, a blog, or only links out \"\n",
    "    \"without listing multiple products, it does NOT count.\\n\\n\"\n",
    "    + \"\\n\".join(lines) +\n",
    "    \"\\nQuestion: Based on these links alone, is this page a central shopping hub \"\n",
    "    \"that aggregates many coffee-bean product listings? Answer 'yes' or 'no' without explanation.\"\n",
    ")\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0, max_tokens=50\n",
    "        )\n",
    "        return \"yes\" in resp.choices[0].message.content.lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in verify_page_sells_coffee_gpt: {e}\")\n",
    "        return False\n",
    "\n",
    "def pick_most_likely_store_page_with_gpt(soup, base_url, visited):\n",
    "    anchors = soup.find_all('a', href=True)[:100]\n",
    "    visited_str = \"\\n\".join(visited) if visited else \"(none)\"\n",
    "    choices = []\n",
    "    for a in anchors:\n",
    "        absu = urljoin(base_url, a['href'])\n",
    "        if absu not in visited:\n",
    "            choices.append(f\"Text: {a.get_text(strip=True)}, URL: {absu}\")\n",
    "    if not choices:\n",
    "        return None\n",
    "    prompt = (\n",
    "        \"Already visited the following URLs, do NOT pick them again:\\n\"\n",
    "        f\"{visited_str}\\n\\n\"\n",
    "        \"Below is a list of anchor texts and their URLs from this webpage. \"\n",
    "        \"Among these unvisited links, find exactly one link that most likely leads \"\n",
    "        \"to an online store selling coffee beans. If none seems relevant, say 'None'.\\n\\n\"\n",
    "        + \"\\n\".join(choices) +\n",
    "        \"\\n your answer should only be the single best URL or 'None' without extra texts.\"\n",
    "    )\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3, max_tokens=150\n",
    "        )\n",
    "        ans = resp.choices[0].message.content.strip()\n",
    "        if \"none\" in ans.lower():\n",
    "            return None\n",
    "        return urljoin(base_url, ans)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in pick_most_likely_store_page_with_gpt: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_coffee_store_page(root_url, max_depth=MAX_DEPTH):\n",
    "    visited = set()\n",
    "    current = root_url\n",
    "    for depth in range(max_depth):\n",
    "        if current in visited:\n",
    "            break\n",
    "        visited.add(current)\n",
    "        print(f\"[Depth {depth+1}] Checking: {current}\")\n",
    "        _, soup = scrape_website(current)\n",
    "        if not soup:\n",
    "            return None\n",
    "        if depth > 0 and verify_page_sells_coffee_gpt(soup, current):\n",
    "            print(f\"Found coffee-selling page at {current}\")\n",
    "            return current\n",
    "        nxt = pick_most_likely_store_page_with_gpt(soup, current, visited)\n",
    "        if not nxt:\n",
    "            return None\n",
    "        current = nxt\n",
    "    return None\n",
    "\n",
    "# ============================\n",
    "# FILTER & WRITE FOR ONE CITY\n",
    "# ============================\n",
    "def filter_coffee_roasters(raw_csv, city):\n",
    "    df = pd.read_csv(raw_csv)\n",
    "    filtered = []\n",
    "    for _, row in df.iterrows():\n",
    "        site = row['Website']\n",
    "        addr = row['Address']\n",
    "        print(f\"\\nProcessing {site}\")\n",
    "        _, soup = scrape_website(site)\n",
    "        if not soup:\n",
    "            continue\n",
    "        resp = analyze_with_gpt(_, addr, city)\n",
    "        if not resp:\n",
    "            continue\n",
    "        offers = False\n",
    "        located = False\n",
    "        for line in resp.split('\\n'):\n",
    "            if line.startswith(\"1.\") and \"yes\" in line:\n",
    "                offers = True\n",
    "            if line.startswith(\"2.\") and \"yes\" in line:\n",
    "                located = True\n",
    "        if offers and located:\n",
    "            store = find_coffee_store_page(site)\n",
    "            if store:\n",
    "                row['Original Website'] = row['Website']   # ← keep homepage\n",
    "                row['Website'] = store                     # ← store page\n",
    "                filtered.append(row)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipped (offers={offers}, located={located}): {site}\")\n",
    "    if filtered:\n",
    "        out_df = pd.DataFrame(filtered)\n",
    "        out_csv = f\"Filtered_{city}_specialty_coffee_roasters.csv\"\n",
    "        out_df.to_csv(out_csv, index=False)\n",
    "        print(f\"[Saved] filtered → {out_csv}\")\n",
    "\n",
    "# ============================\n",
    "# MAIN EXECUTION FLOW\n",
    "# ============================\n",
    "def main():\n",
    "    for city in CITIES:\n",
    "        print(f\"\\n=== PROCESSING CITY: {city} ===\")\n",
    "        raw = scrape_google_places_for_city(city)\n",
    "        filter_coffee_roasters(raw, city)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
