{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b757af-724b-448a-ad1a-a0162ba21bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import openai\n",
    "\n",
    "# Load the CSV file with the store URLs\n",
    "data_path = 'Filtered_Atlanta_specialty_coffee_roasters.csv'\n",
    "store_df = pd.read_csv(data_path)\n",
    "\n",
    "# Replace with your actual API keys\n",
    "GOOGLE_API_KEY = 'your google api key'  # Replace with your actual Google API key\n",
    "OPENAI_API_KEY = 'your openai key'  # Replace with your actual OpenAI API key\n",
    "\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Ensure the CSV has the correct column\n",
    "if 'Website' not in store_df.columns:\n",
    "    raise ValueError(\"The CSV file must contain a 'Website' column.\")\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
    "    })\n",
    "    return session\n",
    "\n",
    "session = create_session()\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "def fetch_product_links(shop_url):\n",
    "    \"\"\"\n",
    "    Use GPT to identify and extract product links from a shop page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(shop_url, timeout=100)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract all anchor tags\n",
    "        anchor_tags = soup.find_all('a', href=True)\n",
    "        anchor_tag_texts = [f\"Text: {a.get_text(strip=True)}, URL: {a['href']}\" for a in anchor_tags]\n",
    "        anchor_tag_text = \"\\n\".join(anchor_tag_texts)\n",
    "        #print(anchor_tag_text)\n",
    "        # Use GPT to identify which links are product links\n",
    "        prompt = f\"\"\"\n",
    "        Below is a list of anchor tags (text and URLs) extracted from a coffee shop website ({shop_url}).\n",
    "        Please identify and return the URLs that are possible(even a bit possible) to lead to individual product pages for coffee. If you are unsure still return it. \n",
    "        Only return a JSON array of valid URLs.\n",
    "\n",
    "        Anchor Tags:\n",
    "        {anchor_tag_text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Extract and clean up GPT response\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        #print(response_text)\n",
    "\n",
    "        # Remove any Markdown code block delimiters and explanatory text\n",
    "        json_match = re.search(r\"\\[.*\\]\", response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group()\n",
    "            product_links = json.loads(json_text)\n",
    "            if isinstance(product_links, list) and all(isinstance(link, str) for link in product_links):\n",
    "                # Convert relative URLs to absolute URLs if needed\n",
    "                absolute_links = [\n",
    "                    urljoin(shop_url, link) if not link.startswith(\"http\") else link\n",
    "                    for link in product_links\n",
    "                ]\n",
    "                return absolute_links\n",
    "            else:\n",
    "                raise ValueError(\"GPT response is not a valid JSON list of URLs.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Failed to extract JSON from GPT response: {response_text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching product links with GPT: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def is_bag_of_coffee(product_url):\n",
    "    \"\"\"\n",
    "    Use GPT to determine if the product is a single bag of roasted coffee.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(product_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract product title and description\n",
    "        title = soup.find('title').get_text(strip=True) if soup.find('title') else \"\"\n",
    "        description = soup.find('meta', {'name': 'description'})\n",
    "        description = description['content'] if description else \"\"\n",
    "\n",
    "        # Combine title and description for analysis\n",
    "        product_text = f\"Title: {title}\\nDescription: {description}\"\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following product details, is this item likely to be a single bag of roasted coffee that is not part of a bundle or subscription and does not include flavored coffees? \n",
    "        Provide a yes or no answer without explanation. Your answer should just be yes or no. \\n\\n{product_text}\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        print(response_text)\n",
    "        return response_text.lower().startswith(\"yes\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error determining product type for {product_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "output_file = 'filtered_coffee_products_Atlanta.csv'\n",
    "\n",
    "coffee_products = []\n",
    "\n",
    "for index, row in store_df.iterrows():\n",
    "    shop_url = row['Website']\n",
    "    print(f\"Processing shop: {shop_url}\")\n",
    "    product_links = fetch_product_links(shop_url)\n",
    "    print(len(product_links))\n",
    "    if product_links:\n",
    "        for product_link in product_links:\n",
    "            if is_bag_of_coffee(product_link):\n",
    "                print(f\"Found coffee bag at: {product_link}\")\n",
    "                coffee_products.append({\n",
    "                    'Shop Website': shop_url,\n",
    "                    'Product Link': product_link\n",
    "                })\n",
    "    else:\n",
    "        print(f\"No product links found for {shop_url}\")\n",
    "\n",
    "coffee_df = pd.DataFrame(coffee_products)\n",
    "coffee_df.to_csv(output_file, index=False)\n",
    "print(f\"Coffee product links saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
